{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/net/pr2/projects/plgrid/plggneuromol/imdik-zekanowski-gts/venv-hail-0.2.105/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-09 16:06:45.244 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.1.3\n",
      "SparkUI available at http://ac0075:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.105-acd89e80c345\n",
      "LOGGING: writing to /net/pr2/projects/plgrid/plggneuromol/imdik-zekanowski-gts/analysis/burden-and-family/hail-20230109-1606-0.2.105-acd89e80c345.log\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import hail as hl\n",
    "\n",
    "hl.init(\n",
    "    tmp_dir='/net/ascratch/people/plggosborcz/gosborcz-hail',\n",
    "    spark_conf={'spark.driver.memory': '30G', 'spark.executor.memory': '30G'},\n",
    "    default_reference='GRCh38') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hail.plot import show\n",
    "from pprint import pprint\n",
    "from bokeh.layouts import gridplot\n",
    "hl.plot.output_notebook()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "\n",
    "from bokeh.plotting import output_notebook, show, figure\n",
    "from bokeh.palettes import viridis\n",
    "\n",
    "output_notebook() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filtering for quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpmk = hl.read_table('/net/archive/groups/plggneuromol/resources/repeatmasker-extended-keyed.ht')\n",
    "\n",
    "cov = hl.read_table('/net/archive/groups/plggneuromol/resources/gnomad/gnomad.genomes.r3.0.1.coverage.ht')\n",
    "cov = cov.filter(cov.over_1 > 0.9)\n",
    "\n",
    "mt = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-unfiltered.mt')\n",
    "mt = mt.filter_rows(hl.is_defined(rpmk[mt.locus]), keep = False)\n",
    "\n",
    "mt.checkpoint('/net/scratch/people/plggosborcz/temp-mts/rpmk.mt')\n",
    "\n",
    "mt = hl.read_matrix_table('/net/scratch/people/plggosborcz/temp-mts/rpmk.mt')\n",
    "mt = mt.naive_coalesce(10000)\n",
    "mt.write('/net/scratch/people/plggosborcz/temp-mts/rpmk-repart-1.mt')\n",
    "\n",
    "mt = hl.read_matrix_table('/net/scratch/people/plggosborcz/temp-mts/rpmk-repart-1.mt')\n",
    "mt.count()\n",
    "\n",
    "mt = mt.filter_rows(hl.is_defined(cov[mt.locus]), keep = True)\n",
    "mt.checkpoint('/net/scratch/people/plggosborcz/temp-mts/gts-cov.mt')\n",
    "\n",
    "mt = hl.read_matrix_table('/net/scratch/people/plggosborcz/temp-mts/gts-cov.mt')\n",
    "mt = mt.naive_coalesce(1000)\n",
    "mt.checkpoint('/net/scratch/people/plggosborcz/temp-mts/gts-cov-repart.mt')\n",
    "\n",
    "mt = mt.annotate_rows(dp_qc = hl.agg.stats(mt.DP),\n",
    "                     gq_qc = hl.agg.stats(mt.GQ),\n",
    "                     hwe = hl.agg.hardy_weinberg_test(mt.GT))\n",
    "\n",
    "mt = mt.annotate_rows(n_below_dp_3 = hl.agg.count_where(mt.DP < 3),\n",
    "                      n_below_gq_30 = hl.agg.count_where(mt.GQ <30))\n",
    "\n",
    "mt.checkpoint('/net/scratch/people/plggosborcz/temp-mts/qc-gts.mt')\n",
    "\n",
    "mt = hl.read_matrix_table('/net/scratch/people/plggosborcz/temp-mts/qc-gts.mt')\n",
    "\n",
    "mt = mt.filter_rows((mt.dp_qc.mean > 5) &\n",
    "                    (mt.gq_qc.mean > 50) &\n",
    "                    (mt.dp_qc.mean < 200) &\n",
    "                    (mt.hwe.p_value > 0.05) &\n",
    "                    (mt.n_below_dp_3 < 3) &\n",
    "                    (mt.n_below_gq_30 < 10))\n",
    "\n",
    "mt.checkpoint('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/gts-fams-filtered.mt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split multiallelic variants and annotate with gnomad and other databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 20:53:54 Hail: INFO: wrote matrix table with 15394580 rows and 124 columns in 1000 partitions to /net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/to-delete.mt\n",
      "2022-05-15 20:56:40 Hail: INFO: wrote matrix table with 15760027 rows and 124 columns in 1000 partitions to /net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-split.mt\n",
      "[Stage 9:==================================>                  (652 + 12) / 1000]\r"
     ]
    }
   ],
   "source": [
    "mt = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/gts-fams-filtered.mt')\n",
    "mt = mt.distinct_by_row()\n",
    "mt.checkpoint('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/to-delete.mt')\n",
    "\n",
    "mt = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/to-delete.mt')\n",
    "\n",
    "mt = mt.key_rows_by(mt.locus, mt.alleles)\n",
    "mt = hl.split_multi_hts(mt)\n",
    "mt.write('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-split.mt')\n",
    "\n",
    "gnomad = hl.read_table('/net/archive/groups/plggneuromol/resources/gnomad/gnomad.genomes.v3.1.1.sites.ht/')\n",
    "\n",
    "mt = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-split.mt')\n",
    "mt = mt.annotate_rows(gnomad_v_3_1 = gnomad[mt.row_key])\n",
    "\n",
    "mt.write('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-gnomad.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-gnomad.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 09:39:03 Hail: INFO: Reading table to impute column types\n",
      "2022-05-16 09:39:05 Hail: INFO: Finished type imputation\n",
      "  Loading field 'f0' as type str (imputed)\n",
      "  Loading field 'f1' as type str (imputed)\n",
      "2022-05-16 09:39:06 Hail: INFO: Ordering unsorted dataset with network shuffle2]\n",
      "2022-05-16 09:39:08 Hail: INFO: Ordering unsorted dataset with network shuffle1]\n",
      "2022-05-16 09:39:09 Hail: INFO: Ordering unsorted dataset with network shuffle2]\n",
      "2022-05-16 09:39:10 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2022-05-16 09:39:11 Hail: INFO: Ordering unsorted dataset with network shuffle1]\n",
      "2022-05-16 09:39:11 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2022-05-16 09:46:34 Hail: INFO: wrote matrix table with 15760027 rows and 124 columns in 1000 partitions to /net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-anno.mt\n",
      "    Total size: 41.48 GiB\n",
      "    * Rows/entries: 41.48 GiB\n",
      "    * Columns: 550.00 B\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 0 rows (20.00 B)\n",
      "    * Largest partition:  23020 rows (131.19 MiB)\n"
     ]
    }
   ],
   "source": [
    "genes = hl.read_table('/net/archive/groups/plggneuromol/resources/genecode_v32.ht')\n",
    "genes = genes.filter(hl.is_valid_contig(genes['hg38.knownGene.chrom'], reference_genome='GRCh38'))\n",
    "\n",
    "hpo = hl.import_table('/net/archive/groups/plggneuromol/resources/hpo.tsv', impute = True, no_header=True)\n",
    "\n",
    "start = genes['hg38.knownGene.txStart']\n",
    "stop =  genes['hg38.knownGene.txEnd']\n",
    "\n",
    "genes = genes.transmute(interval = \n",
    "                        hl.locus_interval(genes['hg38.knownGene.chrom'], \n",
    "                                          start,\n",
    "                                          stop,\n",
    "                                          reference_genome='GRCh38', includes_start=False))\n",
    "\n",
    "genes = genes.key_by(genes['hg38.kgXref.geneSymbol'])\n",
    "\n",
    "hpo = hpo.key_by(hpo.f0)\n",
    "\n",
    "genes = genes.annotate(hpo = hpo.index(genes['hg38.kgXref.geneSymbol'], all_matches = True)['f1'])\n",
    "genes = genes.key_by(genes.interval)\n",
    "\n",
    "mt = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-gnomad.mt')\n",
    "\n",
    "mt = mt.annotate_rows(within_gene = hl.array(hl.set(genes.index(mt.locus, all_matches=True)['hg38.kgXref.geneSymbol'])))\n",
    "mt = mt.annotate_rows(hpo = hl.array(hl.set(genes.index(mt.locus, all_matches=True)['hpo'])))\n",
    "mt.write('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-anno.mt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotate with phenotypes + create a separate mt for each of the larger families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-anno.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 13:25:18 Hail: INFO: Reading table to impute column types\n",
      "2022-05-17 13:25:19 Hail: INFO: Finished type imputation\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'family' as type str (imputed)\n",
      "  Loading field 'sex' as type str (imputed)\n",
      "  Loading field 'kinship' as type str (imputed)\n",
      "  Loading field 'disease' as type str (imputed)\n",
      "  Loading field 'phenotype' as type str (imputed)\n",
      "  Loading field 'add_pheno' as type str (imputed)\n",
      "  Loading field 'heavy_tics' as type str (imputed)\n",
      "  Loading field 'heavy_tics_familial' as type str (imputed)\n",
      "  Loading field 'GTS_ASD_group' as type str (imputed)\n",
      "  Loading field 'nonCTD' as type str (imputed)\n"
     ]
    }
   ],
   "source": [
    "pheno = hl.import_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/pheno/GTS-coded-corrected-june-2021.csv', impute = True, key='ID', delimiter = ',', quote =\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_cols(phenotypes = pheno[mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. For each of the families get intragenic variants with cadd > 5 for further filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fams = list(set(mt.phenotypes.family.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fams.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadd = hl.read_table('/net/archive/groups/plggneuromol/resources/cadd.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 11:42:00 Hail: INFO: wrote matrix table with 343873 rows and 7 columns in 50 partitions to /net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-filteredA.mt\n",
      "    Total size: 2.34 GiB\n",
      "    * Rows/entries: 2.34 GiB\n",
      "    * Columns: 166.00 B\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 21 rows (125.68 KiB)\n",
      "    * Largest partition:  9745 rows (68.39 MiB)\n",
      "2022-05-16 11:56:59 Hail: INFO: wrote matrix table with 363663 rows and 8 columns in 50 partitions to /net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-filteredB.mt\n",
      "    Total size: 2.47 GiB\n",
      "    * Rows/entries: 2.47 GiB\n",
      "    * Columns: 206.00 B\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 31 rows (134.79 KiB)\n",
      "    * Largest partition:  10002 rows (70.27 MiB)\n",
      "[Stage 8:>                                                        (0 + 12) / 50]\r"
     ]
    }
   ],
   "source": [
    "for f in fams:\n",
    "    fam = mt.filter_cols(mt.phenotypes.family == f)\n",
    "    fam = fam.filter_rows(hl.agg.any(fam.GT.is_non_ref()))\n",
    "    fam = fam.naive_coalesce(50)\n",
    "    fam = fam.filter_rows(fam.within_gene == hl.empty_array(hl.tstr), keep = False)\n",
    "    fam = fam.annotate_rows(cadd = cadd[fam.row_key])\n",
    "    fam = fam.filter_rows(fam.cadd.score_phred > 5)\n",
    "    fam.write('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-filtered'+f+'.mt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional annotation with vep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vep = hl.read_table('/net/archive/groups/plggneuromol/resources/vep38/grch38_context_vep_annotated.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 12) / 50]\r"
     ]
    }
   ],
   "source": [
    "for f in fams:\n",
    "    fam = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-filtered'+f+'.mt')\n",
    "    fam = fam.annotate_rows(vep = vep[fam.row_key])\n",
    "    fam.write('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-vep'+f+'.mt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. For each of the families find pathogenic variants according to mutation schema:\n",
    "\n",
    "* first in all of the genes (maf < 0.001 or not in Gnomad, cadd > 10)\n",
    "* then in selected genes (maf < 0.05 or not in Gnomad, cadd > 10) \n",
    "* ten in all of the genes (maf < 0.01 or not in Gnomad, cadd > 5)\n",
    "* then in selected genes (maf < 0.1 or not in Gnomad, cadd > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/external-data/burden-and-family/all-genes-burde-and-family.csv', names=['gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes = set(genes.gene)\n",
    "all_genes = list({x.replace('\\xa0\\xa0\\xa0', '') for x in all_genes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. For each of the larger families find pathogenic variants according to the schema by KF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 21:14:07 Hail: INFO: Reading table to impute column types\n",
      "2022-06-22 21:14:08 Hail: WARN: Name collision: field 'sample' already in object dict. \n",
      "  This field must be referenced with __getitem__ syntax: obj['sample']\n",
      "2022-06-22 21:14:08 Hail: INFO: Finished type imputation\n",
      "  Loading field 'sample' as type str (imputed)\n",
      "  Loading field 'family' as type str (imputed)\n",
      "  Loading field 'mutation_option_1' as type str (imputed)\n",
      "  Loading field 'mutation_option_2' as type str (imputed)\n"
     ]
    }
   ],
   "source": [
    "schema = hl.import_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/external-data/burden-and-family/familiespattern.csv', impute = True, key='sample', delimiter = '\\;', quote =\"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Export a table with genes that are duplicated \n",
    "- this is first done withou the T and H families \n",
    "(these are double schema so they are dine in the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fams = ['A', 'B', 'C','D', 'E', 'F', 'G', 'I', 'J', 'R', 'S', 'U', 'W', 'X', 'Y'] \n",
    "\n",
    "fams_all_genes = ['A_all_genes', 'B_all_genes', 'C_all_genes','D_all_genes',\n",
    "                  'E_all_genes', 'F_all_genes', 'G_all_genes', \n",
    "                  'I_all_genes', 'J_all_genes', 'R_all_genes', 'S_all_genes',\n",
    "                  'U_all_genes', 'W_all_genes', 'X_all_genes', 'Y_all_genes']\n",
    "\n",
    "fams_selected_genes = ['A_selected_genes', 'B_selected_genes', 'C_selected_genes','D_selected_genes',\n",
    "                  'E_selected_genes', 'F_selected_genes', 'G_selected_genes',\n",
    "                  'I_selected_genes', 'J_selected_genes', 'R_selected_genes', 'S_selected_genes',\n",
    "                  'U_selected_genes', 'W_selected_genes', 'X_selected_genes', 'Y_selected_genes']\n",
    "\n",
    "fams_all_genes_other_treshold = ['A_all_genes_lower_treshold', 'B_all_genes_lower_treshold',\n",
    "                                 'C_all_genes_lower_treshold','D_all_genes_lower_treshold',\n",
    "                  'E_all_genes_lower_treshold', 'F_all_genes_lower_treshold', 'G_all_genes_lower_treshold', \n",
    "                  'I_all_genes_lower_treshold', 'J_all_genes_lower_treshold', 'R_all_genes_lower_treshold',\n",
    "                   'S_all_genes_lower_treshold','U_all_genes_lower_treshold', 'W_all_genes_lower_treshold',\n",
    "                   'X_all_genes_lower_treshold', 'Y_all_genes_lower_treshold']\n",
    "\n",
    "fams_selected_genes_other_treshold = ['A_selected_genes_lower_treshold', 'B_selected_genes_lower_treshold',\n",
    "                                      'C_selected_genes_lower_treshold','D_selected_genes_lower_treshold',\n",
    "                  'E_selected_genes_lower_treshold', 'F_selected_genes_lower_treshold', 'G_selected_genes_lower_treshold',\n",
    "                  'I_selected_genes_lower_treshold', 'J_selected_genes_lower_treshold', 'R_selected_genes_lower_treshold',\n",
    "                                      'S_selected_genes_lower_treshold',\n",
    "                  'U_selected_genes_lower_treshold', 'W_selected_genes_lower_treshold', 'X_selected_genes_lower_treshold',\n",
    "                                      'Y_selected_genes_lower_treshold']\n",
    "\n",
    "fams_description = ['A_description', 'B_description', 'C_description','D_description',\n",
    "                  'E_description', 'F_description', 'G_description',\n",
    "                  'I_description', 'J_description', 'R_description', 'S_description',\n",
    "                  'U_description', 'W_description', 'X_description', 'Y_description']\n",
    "\n",
    "fams1 = []\n",
    "fams2 = []\n",
    "fams3 = []\n",
    "fams4 = []\n",
    "fams_to_export_all = []\n",
    "fams_to_export_selected = []\n",
    "fams_to_export_all_other_treshold = []\n",
    "fams_to_export_selected_other_treshold = []\n",
    "descriptions_to_export = []\n",
    "\n",
    "genes_fams1 = []\n",
    "genes_fams2 = []\n",
    "genes_fams3 = []\n",
    "genes_fams4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 152:=================================================>     (45 + 5) / 50]\r"
     ]
    }
   ],
   "source": [
    "for f in fams:\n",
    "    fam = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-vep'+f+'.mt')\n",
    "    fam = fam.annotate_cols(schema = schema[fam.s])\n",
    "    fam = fam.annotate_rows(segregation = hl.agg.group_by(fam.schema.mutation_option_1, hl.agg.count_where(fam.GT.is_non_ref())))\n",
    "    with_mutation = fam.aggregate_cols((hl.agg.counter(fam.schema.mutation_option_1))['YES'])\n",
    "    fam = fam.filter_rows((fam.segregation.get('NO', 0) == 0) & (fam.segregation.get('YES', 0) == with_mutation))\n",
    "    \n",
    "    fam = fam.filter_rows(hl.is_snp(fam.alleles[0], fam.alleles[1]))\n",
    "    \n",
    "    fam1 = fam.filter_rows(\n",
    "        (fam.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fam3 = fam.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.01,\n",
    "            True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    genes_fams1.append(fam1.within_gene.collect())\n",
    "    genes_fams3.append(fam3.within_gene.collect())\n",
    "    \n",
    "    fams1.append(fam1)\n",
    "    fams3.append(fam3)\n",
    "    \n",
    "    fam2 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "    fam4 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "    \n",
    "    fam2 = fam2.filter_rows(\n",
    "        (fam2.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam2.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam2.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    "    )\n",
    "    fam4 = fam4.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam4.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam4.gnomad_v_3_1.freq.AF[2] < 0.1,\n",
    "            True\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    genes_fams2.append(fam2.within_gene.collect())\n",
    "    genes_fams4.append(fam4.within_gene.collect())\n",
    "    \n",
    "    fams2.append(fam2)\n",
    "    fams4.append(fam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 232:=====================================================> (49 + 1) / 50]\r"
     ]
    }
   ],
   "source": [
    "#add H1, H2 and T1, T2 options:\n",
    "\n",
    "fams1_h = []\n",
    "fams2_h = []\n",
    "fams3_h = []\n",
    "fams4_h = []\n",
    "\n",
    "genes_fams1_h = []\n",
    "genes_fams2_h = []\n",
    "genes_fams3_h = []\n",
    "genes_fams4_h = []\n",
    "\n",
    "fam = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-vepH.mt')\n",
    "fam = fam.annotate_cols(schema = schema[fam.s])\n",
    "fam = fam.annotate_rows(segregation = hl.agg.group_by(fam.schema.mutation_option_1, hl.agg.count_where(fam.GT.is_non_ref())))\n",
    "with_mutation = fam.aggregate_cols((hl.agg.counter(fam.schema.mutation_option_1))['YES'])\n",
    "fam = fam.filter_rows((fam.segregation.get('NO', 0) == 0) & (fam.segregation.get('YES', 0) == with_mutation))\n",
    "fam = fam.filter_rows(hl.is_snp(fam.alleles[0], fam.alleles[1]))\n",
    "\n",
    "fam1 = fam.filter_rows(\n",
    "        (fam.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    ")\n",
    "    \n",
    "fam3 = fam.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.01,\n",
    "            True\n",
    "        )\n",
    ")\n",
    "\n",
    "genes_fams1_h.append(fam1.within_gene.collect())\n",
    "genes_fams3_h.append(fam3.within_gene.collect())\n",
    "\n",
    "fams1_h.append(fam1)\n",
    "fams3_h.append(fam3)\n",
    "   \n",
    "fam2 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "fam4 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "\n",
    "fam2 = fam2.filter_rows(\n",
    "        (fam2.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam2.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam2.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    "    )\n",
    "fam4 = fam4.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam4.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam4.gnomad_v_3_1.freq.AF[2] < 0.1,\n",
    "            True\n",
    "        )\n",
    "    )\n",
    "\n",
    "genes_fams2_h.append(fam2.within_gene.collect())\n",
    "fams2_h.append(fam2)\n",
    "\n",
    "genes_fams4_h.append(fam4.within_gene.collect())\n",
    "fams4_h.append(fam4)\n",
    "\n",
    "\n",
    "fam = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-vepH2.mt')\n",
    "fam = fam.annotate_cols(schema = schema[fam.s])\n",
    "fam = fam.annotate_rows(segregation = hl.agg.group_by(fam.schema.mutation_option_2, hl.agg.count_where(fam.GT.is_non_ref())))\n",
    "with_mutation = fam.aggregate_cols((hl.agg.counter(fam.schema.mutation_option_2))['YES'])\n",
    "fam = fam.filter_rows((fam.segregation.get('NO', 0) == 0) & (fam.segregation.get('YES', 0) == with_mutation))    \n",
    "fam = fam.filter_rows(hl.is_snp(fam.alleles[0], fam.alleles[1]))\n",
    "    \n",
    "fam1 = fam.filter_rows(\n",
    "        (fam.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.001,\n",
    "            True)\n",
    "        )\n",
    ")\n",
    "    \n",
    "fam3 = fam.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.01,\n",
    "            True\n",
    "        )\n",
    ")\n",
    "\n",
    "genes_fams1_h.append(fam1.within_gene.collect())\n",
    "genes_fams3_h.append(fam3.within_gene.collect())\n",
    "\n",
    "fams1_h.append(fam1)\n",
    "fams3_h.append(fam3)\n",
    "   \n",
    "fam2 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "fam4 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "\n",
    "fam2 = fam2.filter_rows(\n",
    "        (fam2.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam2.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam2.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    "    )\n",
    "fam4 = fam4.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam4.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam4.gnomad_v_3_1.freq.AF[2] < 0.1,\n",
    "            True\n",
    "        )\n",
    "    )\n",
    "\n",
    "genes_fams2_h.append(fam2.within_gene.collect())\n",
    "genes_fams4_h.append(fam4.within_gene.collect())\n",
    "\n",
    "fams4_h.append(fam4)\n",
    "fams2_h.append(fam2)\n",
    "\n",
    "\n",
    "fams1_t = []\n",
    "fams2_t = []\n",
    "fams3_t = []\n",
    "fams4_t = []\n",
    "\n",
    "genes_fams1_t = []\n",
    "genes_fams2_t = []\n",
    "genes_fams3_t = []\n",
    "genes_fams4_t = []\n",
    "\n",
    "fam = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-vepT.mt')\n",
    "fam = fam.annotate_cols(schema = schema[fam.s])\n",
    "fam = fam.annotate_rows(segregation = hl.agg.group_by(fam.schema.mutation_option_1, hl.agg.count_where(fam.GT.is_non_ref())))\n",
    "with_mutation = fam.aggregate_cols((hl.agg.counter(fam.schema.mutation_option_1))['YES'])\n",
    "fam = fam.filter_rows((fam.segregation.get('NO', 0) == 0) & (fam.segregation.get('YES', 0) == with_mutation))\n",
    "fam = fam.filter_rows(hl.is_snp(fam.alleles[0], fam.alleles[1]))\n",
    "    \n",
    "fam1 = fam.filter_rows(\n",
    "        (fam.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.001,\n",
    "            True)\n",
    "        )\n",
    ")\n",
    "    \n",
    "fam3 = fam.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.01,\n",
    "            True\n",
    "        )\n",
    ")\n",
    "\n",
    "genes_fams1_t.append(fam1.within_gene.collect())\n",
    "genes_fams3_t.append(fam3.within_gene.collect())\n",
    "\n",
    "fams1_t.append(fam1)\n",
    "fams3_t.append(fam3)\n",
    "   \n",
    "fam2 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "fam4 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "\n",
    "fam2 = fam2.filter_rows(\n",
    "        (fam2.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam2.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam2.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    "    )\n",
    "fam4 = fam4.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam4.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam4.gnomad_v_3_1.freq.AF[2] < 0.1,\n",
    "            True\n",
    "        )\n",
    "    )\n",
    "\n",
    "genes_fams2_t.append(fam2.within_gene.collect())\n",
    "genes_fams4_t.append(fam4.within_gene.collect())\n",
    "\n",
    "fams4_t.append(fam4)\n",
    "fams2_t.append(fam2)\n",
    "\n",
    "fam = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-vepT2.mt')\n",
    "fam = fam.annotate_cols(schema = schema[fam.s])\n",
    "fam = fam.annotate_rows(segregation = hl.agg.group_by(fam.schema.mutation_option_2, hl.agg.count_where(fam.GT.is_non_ref())))\n",
    "with_mutation = fam.aggregate_cols((hl.agg.counter(fam.schema.mutation_option_2))['YES'])\n",
    "fam = fam.filter_rows((fam.segregation.get('NO', 0) == 0) & (fam.segregation.get('YES', 0) == with_mutation))    \n",
    "fam = fam.filter_rows(hl.is_snp(fam.alleles[0], fam.alleles[1]))\n",
    "    \n",
    "fam1 = fam.filter_rows(\n",
    "        (fam.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    ")\n",
    "    \n",
    "fam3 = fam.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam.gnomad_v_3_1.freq.AF[2] < 0.01,\n",
    "            True\n",
    "        )\n",
    ")\n",
    "\n",
    "genes_fams1_t.append(fam1.within_gene.collect())\n",
    "genes_fams3_t.append(fam3.within_gene.collect())\n",
    "\n",
    "fams1_t.append(fam1)\n",
    "fams3_t.append(fam3)\n",
    "   \n",
    "fam2 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "fam4 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "\n",
    "fam2 = fam2.filter_rows(\n",
    "        (fam2.cadd.score_phred > 10) &\n",
    "        (hl.if_else(\n",
    "            hl.is_defined(fam2.gnomad_v_3_1.freq.AF[2]), \n",
    "            fam2.gnomad_v_3_1.freq.AF[2] < 0.05,\n",
    "            True)\n",
    "        )\n",
    "    )\n",
    "fam4 = fam4.filter_rows(\n",
    "        hl.if_else(\n",
    "            hl.is_defined(fam4.gnomad_v_3_1.freq.AF[2]),\n",
    "            fam4.gnomad_v_3_1.freq.AF[2] < 0.1,\n",
    "            True\n",
    "        )\n",
    "    )\n",
    "\n",
    "genes_fams2_t.append(fam2.within_gene.collect())\n",
    "genes_fams4_t.append(fam4.within_gene.collect())\n",
    "\n",
    "fams4_t.append(fam4)\n",
    "fams2_t.append(fam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is an optional step to get only variants that are duplicated\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams1_h):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams1_h[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams1_t):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams1_t[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams1):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams1[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams3_h):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams3_h[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams3_t):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams3_t[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams3):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams3[idx] = list(set(gene_list))\n",
    "\n",
    "genes_fams1_h = [item for sublist in genes_fams1_h for item in sublist] \n",
    "genes_fams1_t = [item for sublist in genes_fams1_t for item in sublist]\n",
    "genes_fams1 = [item for sublist in genes_fams1 for item in sublist]\n",
    "\n",
    "genes_fams3_h = [item for sublist in genes_fams3_h for item in sublist] \n",
    "genes_fams3_t = [item for sublist in genes_fams3_t for item in sublist]\n",
    "genes_fams3 = [item for sublist in genes_fams3 for item in sublist]\n",
    "\n",
    "# now get only unique items from genes_fams1_h and genes_fams1_t:\n",
    "genes_fams1_h = list(set(genes_fams1_h))\n",
    "genes_fams1_t = list(set(genes_fams1_t))\n",
    "\n",
    "genes_fams3_h = list(set(genes_fams3_h))\n",
    "genes_fams3_t = list(set(genes_fams3_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_fams1 = genes_fams1 + genes_fams1_h + genes_fams1_t\n",
    "genes_fams3 = genes_fams3 + genes_fams3_h + genes_fams3_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "dupes_1 = [x for x in genes_fams1 if x in seen or seen.add(x)]    \n",
    "len(set(dupes_1)) #this should contain all the genes that repeat themselfs excluding genes common for H1 and H2 and T1 and T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1575"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "dupes_3 = [x for x in genes_fams3 if x in seen or seen.add(x)]    \n",
    "len(set(dupes_3)) #this should contain all the genes that repeat themselfs excluding genes common for H1 and H2 and T1 and T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, gene_list in enumerate(genes_fams2_h):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams2_h[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams2_t):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams2_t[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams2):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams2[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams4_h):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams4_h[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams4_t):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams4_t[idx] = list(set(gene_list))\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams4):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family and remove variants that are double within a family\n",
    "    genes_fams4[idx] = list(set(gene_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_fams2_h = [item for sublist in genes_fams2_h for item in sublist] \n",
    "genes_fams2_t = [item for sublist in genes_fams2_t for item in sublist]\n",
    "genes_fams2 = [item for sublist in genes_fams2 for item in sublist]\n",
    "genes_fams4_h = [item for sublist in genes_fams4_h for item in sublist] \n",
    "genes_fams4_t = [item for sublist in genes_fams4_t for item in sublist]\n",
    "genes_fams4 = [item for sublist in genes_fams4 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get only unique items from genes_fams1_h and genes_fams1_t:\n",
    "genes_fams2_h = list(set(genes_fams2_h))\n",
    "genes_fams2_t = list(set(genes_fams2_t))\n",
    "genes_fams2 = genes_fams2 + genes_fams2_h + genes_fams2_t\n",
    "\n",
    "genes_fams4_h = list(set(genes_fams4_h))\n",
    "genes_fams4_t = list(set(genes_fams4_t))\n",
    "genes_fams4 = genes_fams4 + genes_fams4_h + genes_fams4_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "dupes_2 = [x for x in genes_fams2 if x in seen or seen.add(x)]    \n",
    "len(set(dupes_2)) #this should contain all the genes that repeat themselfs excluding genes common for H1 and H2 and T1 and T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "dupes_4 = [x for x in genes_fams4 if x in seen or seen.add(x)]    \n",
    "len(set(dupes_4)) #this should contain all the genes that repeat themselfs excluding genes common for H1 and H2 and T1 and T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now append h1 h2 and t1 t2 to fams1\n",
    "fams.append('H1')\n",
    "fams.append('H2')\n",
    "fams_all_genes.append('H1_all_genes')\n",
    "fams_all_genes.append('H2_all_genes')\n",
    "fams_selected_genes.append('H1_selected_genes')\n",
    "fams_selected_genes.append('H2_selected_genes')\n",
    "fams_all_genes_other_treshold.append('H1_all_genes_lower_treshold')\n",
    "fams_all_genes_other_treshold.append('H2_all_genes_lower_treshold')\n",
    "fams_selected_genes_other_treshold.append('H1_selected_genes_lower_treshold')\n",
    "fams_selected_genes_other_treshold.append('H2_selected_genes_lower_treshold')\n",
    "fams_description.append('H1_description')\n",
    "fams_description.append('H2_description')\n",
    "\n",
    "fams.append('T1')\n",
    "fams.append('T2')\n",
    "fams_all_genes.append('T1_all_genes')\n",
    "fams_all_genes.append('T2_all_genes')\n",
    "fams_selected_genes.append('T1_selected_genes')\n",
    "fams_selected_genes.append('T2_selected_genes')\n",
    "fams_all_genes_other_treshold.append('T1_all_genes_lower_treshold')\n",
    "fams_all_genes_other_treshold.append('T2_all_genes_lower_treshold')\n",
    "fams_selected_genes_other_treshold.append('T1_selected_genes_lower_treshold')\n",
    "fams_selected_genes_other_treshold.append('T2_selected_genes_lower_treshold')\n",
    "fams_description.append('T1_description')\n",
    "fams_description.append('T2_description')\n",
    "\n",
    "fams1 = fams1 + fams1_h + fams1_t\n",
    "fams2 = fams2 + fams2_h + fams2_t\n",
    "fams3 = fams3 + fams3_h + fams3_t\n",
    "fams4 = fams4 + fams4_h + fams4_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fams2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fams3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fams4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now filter all the families only to have variants with duplicated genes\n",
    "for idx, fam in enumerate(fams1):\n",
    "    fam = fam.filter_rows((hl.any(lambda x: hl.literal(dupes_1).contains(x), fam.within_gene)))\n",
    "    fams1[idx] = fam\n",
    "\n",
    "for idx, fam in enumerate(fams2):\n",
    "    fam = fam.filter_rows((hl.any(lambda x: hl.literal(dupes_2).contains(x), fam.within_gene)))\n",
    "    fams2[idx] = fam\n",
    "    \n",
    "for idx, fam in enumerate(fams3):\n",
    "    fam = fam.filter_rows((hl.any(lambda x: hl.literal(dupes_3).contains(x), fam.within_gene)))\n",
    "    fams3[idx] = fam\n",
    "\n",
    "for idx, fam in enumerate(fams4):\n",
    "    fam = fam.filter_rows((hl.any(lambda x: hl.literal(dupes_4).contains(x), fam.within_gene)))\n",
    "    fams4[idx] = fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 20:48:06 Hail: INFO: merging 51 files totalling 3.9M...(49 + 1) / 50]\n",
      "2022-05-21 20:48:06 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 388.133ms\n",
      "2022-05-21 20:48:14 Hail: INFO: merging 51 files totalling 396.3K...9 + 1) / 50]\n",
      "2022-05-21 20:48:14 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 51.441ms\n",
      "2022-05-21 20:48:22 Hail: INFO: merging 51 files totalling 238.8K...0 + 0) / 50]\n",
      "2022-05-21 20:48:22 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.101ms\n",
      "2022-05-21 20:48:30 Hail: INFO: merging 51 files totalling 1.6M...(49 + 1) / 50]\n",
      "2022-05-21 20:48:30 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 52.085ms\n",
      "2022-05-21 20:48:38 Hail: INFO: merging 51 files totalling 2.1M...(48 + 2) / 50]\n",
      "2022-05-21 20:48:38 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 53.743ms\n",
      "2022-05-21 20:48:46 Hail: INFO: merging 51 files totalling 274.7K...9 + 1) / 50]\n",
      "2022-05-21 20:48:47 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 50.186ms\n",
      "2022-05-21 20:48:54 Hail: INFO: merging 51 files totalling 6.6M...(49 + 1) / 50]\n",
      "2022-05-21 20:48:54 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 67.461ms\n",
      "2022-05-21 20:49:02 Hail: INFO: merging 51 files totalling 2.5M...(49 + 1) / 50]\n",
      "2022-05-21 20:49:02 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 55.604ms\n",
      "2022-05-21 20:49:10 Hail: INFO: merging 51 files totalling 9.5M...(45 + 5) / 50]\n",
      "2022-05-21 20:49:10 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 75.812ms\n",
      "2022-05-21 20:49:18 Hail: INFO: merging 51 files totalling 1.5M...(48 + 2) / 50]\n",
      "2022-05-21 20:49:18 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 56.878ms\n",
      "2022-05-21 20:49:26 Hail: INFO: merging 51 files totalling 3.4M...(47 + 3) / 50]\n",
      "2022-05-21 20:49:26 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 59.077ms\n",
      "2022-05-21 20:49:35 Hail: INFO: merging 51 files totalling 2.2M...(49 + 1) / 50]\n",
      "2022-05-21 20:49:35 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 54.421ms\n",
      "2022-05-21 20:49:43 Hail: INFO: merging 51 files totalling 153.7K...9 + 1) / 50]\n",
      "2022-05-21 20:49:43 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.165ms\n",
      "2022-05-21 20:49:51 Hail: INFO: merging 51 files totalling 6.6M...(49 + 1) / 50]\n",
      "2022-05-21 20:49:51 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 66.158ms\n",
      "2022-05-21 20:49:59 Hail: INFO: merging 51 files totalling 1.3M...(49 + 1) / 50]\n",
      "2022-05-21 20:49:59 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 52.091ms\n",
      "2022-05-21 20:50:07 Hail: INFO: merging 51 files totalling 4.3M...(49 + 1) / 50]\n",
      "2022-05-21 20:50:07 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 60.624ms\n",
      "2022-05-21 20:50:14 Hail: INFO: merging 51 files totalling 390.0K...9 + 1) / 50]\n",
      "2022-05-21 20:50:15 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 55.328ms\n",
      "2022-05-21 20:50:21 Hail: INFO: merging 51 files totalling 1.1M...(49 + 1) / 50]\n",
      "2022-05-21 20:50:21 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 53.991ms\n",
      "2022-05-21 20:50:29 Hail: INFO: merging 51 files totalling 2.5M...(49 + 1) / 50]\n",
      "2022-05-21 20:50:29 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 54.744ms\n"
     ]
    }
   ],
   "source": [
    "for fam in fams1:\n",
    "    fam = fam.drop(fam['a_index'], fam['was_split'])\n",
    "    #also annotate rows with family\n",
    "    fam = fam.transmute_rows(DP_stats = fam.dp_qc,\n",
    "                             GQ_stats =fam.gq_qc, \n",
    "                             gnomad_v3_nfe_af = fam.gnomad_v_3_1.freq.AF[2],\n",
    "                             gnomad_v3_nfe_homozygote_count = fam.gnomad_v_3_1.freq.homozygote_count[2],\n",
    "                             cadd = fam.cadd.score_phred,\n",
    "                             within_gene = fam.within_gene,\n",
    "                             hpo = fam.hpo,\n",
    "                             non_refs_healthy = fam.segregation.get('NO', 0),\n",
    "                             non_refs_gts = fam.segregation.get('YES', 0),\n",
    "                             most_severe_consequence = fam.vep.vep.most_severe_consequence,\n",
    "                             transcript_consequences = fam.vep.vep.transcript_consequences,\n",
    "                             intergenic_consequences = fam.vep.vep.intergenic_consequences,\n",
    "                             motif_feature_consequences = fam.vep.vep.motif_feature_consequences,\n",
    "                             regulatory_feature_consequences = fam.vep.vep.regulatory_feature_consequences,\n",
    "                             family_non_ref = hl.agg.filter(hl.any(fam.GT.is_non_ref()), hl.agg.collect(fam.phenotypes.family))[0])\n",
    "    fam = fam.select_entries(fam.GT)\n",
    "    #fam.make_table().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t')\n",
    "    fam.rows().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t')\n",
    "    fams_to_export_all.append(pd.read_csv('/net/scratch/people/plggosborcz/temp-mts/temp.csv', sep ='\\t')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 20:50:29 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n",
      "    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n",
      "2022-05-21 20:50:30 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:31 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:31 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:32 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:33 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:34 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:34 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:35 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:36 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:37 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:37 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:38 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:39 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:39 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:40 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:41 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:42 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:42 Hail: INFO: Coerced sorted dataset\n",
      "2022-05-21 20:50:43 Hail: INFO: Coerced sorted dataset\n"
     ]
    }
   ],
   "source": [
    "for fam in fams1:\n",
    "    descriptions_to_export.append(fam.cols().to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 20:50:51 Hail: INFO: merging 51 files totalling 3.9M...(49 + 1) / 50]\n",
      "2022-05-21 20:50:51 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 60.152ms\n",
      "2022-05-21 20:50:59 Hail: INFO: merging 51 files totalling 348.3K...9 + 1) / 50]\n",
      "2022-05-21 20:50:59 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.599ms\n",
      "2022-05-21 20:51:07 Hail: INFO: merging 51 files totalling 163.9K...9 + 1) / 50]\n",
      "2022-05-21 20:51:07 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.463ms\n",
      "2022-05-21 20:51:14 Hail: INFO: merging 51 files totalling 1.5M...(49 + 1) / 50]\n",
      "2022-05-21 20:51:15 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 55.251ms\n",
      "2022-05-21 20:51:22 Hail: INFO: merging 51 files totalling 2.0M...(48 + 2) / 50]\n",
      "2022-05-21 20:51:22 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 54.506ms\n",
      "2022-05-21 20:51:30 Hail: INFO: merging 51 files totalling 232.1K...9 + 1) / 50]\n",
      "2022-05-21 20:51:30 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.509ms\n",
      "2022-05-21 20:51:37 Hail: INFO: merging 51 files totalling 7.2M...(49 + 1) / 50]\n",
      "2022-05-21 20:51:37 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 70.507ms\n",
      "2022-05-21 20:51:45 Hail: INFO: merging 51 files totalling 1.9M...(49 + 1) / 50]\n",
      "2022-05-21 20:51:45 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 55.907ms\n",
      "2022-05-21 20:51:53 Hail: INFO: merging 51 files totalling 9.2M...(48 + 2) / 50]\n",
      "2022-05-21 20:51:53 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 73.729ms\n",
      "2022-05-21 20:52:00 Hail: INFO: merging 51 files totalling 1.1M...(46 + 4) / 50]\n",
      "2022-05-21 20:52:00 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 53.465ms\n",
      "2022-05-21 20:52:08 Hail: INFO: merging 51 files totalling 3.2M...(48 + 2) / 50]\n",
      "2022-05-21 20:52:08 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 54.185ms\n",
      "2022-05-21 20:52:17 Hail: INFO: merging 51 files totalling 2.3M...(49 + 1) / 50]\n",
      "2022-05-21 20:52:17 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 53.273ms\n",
      "2022-05-21 20:52:25 Hail: INFO: merging 51 files totalling 260.1K...8 + 2) / 50]\n",
      "2022-05-21 20:52:25 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.451ms\n",
      "2022-05-21 20:52:32 Hail: INFO: merging 51 files totalling 7.2M...(49 + 1) / 50]\n",
      "2022-05-21 20:52:32 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 64.432ms\n",
      "2022-05-21 20:52:40 Hail: INFO: merging 51 files totalling 1.5M...(49 + 1) / 50]\n",
      "2022-05-21 20:52:40 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 55.826ms\n",
      "2022-05-21 20:52:48 Hail: INFO: merging 51 files totalling 4.3M...(49 + 1) / 50]\n",
      "2022-05-21 20:52:48 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 60.063ms\n",
      "2022-05-21 20:52:56 Hail: INFO: merging 51 files totalling 2.1M...(49 + 1) / 50]\n",
      "2022-05-21 20:52:56 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 56.041ms\n",
      "2022-05-21 20:53:03 Hail: INFO: merging 51 files totalling 6.1M...(49 + 1) / 50]\n",
      "2022-05-21 20:53:03 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 64.017ms\n",
      "2022-05-21 20:53:10 Hail: INFO: merging 51 files totalling 2.7M...(49 + 1) / 50]\n",
      "2022-05-21 20:53:10 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 58.307ms\n"
     ]
    }
   ],
   "source": [
    "for fam in fams3:\n",
    "    fam = fam.drop(fam['a_index'], fam['was_split'])\n",
    "    #also annotate rows with family\n",
    "    fam = fam.transmute_rows(DP_stats = fam.dp_qc,\n",
    "                             GQ_stats =fam.gq_qc, \n",
    "                             gnomad_v3_nfe_af = fam.gnomad_v_3_1.freq.AF[2],\n",
    "                             gnomad_v3_nfe_homozygote_count = fam.gnomad_v_3_1.freq.homozygote_count[2],\n",
    "                             cadd = fam.cadd.score_phred,\n",
    "                             within_gene = fam.within_gene,\n",
    "                             hpo = fam.hpo,\n",
    "                             non_refs_healthy = fam.segregation.get('NO', 0),\n",
    "                             non_refs_gts = fam.segregation.get('YES', 0),\n",
    "                             most_severe_consequence = fam.vep.vep.most_severe_consequence,\n",
    "                             transcript_consequences = fam.vep.vep.transcript_consequences,\n",
    "                             intergenic_consequences = fam.vep.vep.intergenic_consequences,\n",
    "                             motif_feature_consequences = fam.vep.vep.motif_feature_consequences,\n",
    "                             regulatory_feature_consequences = fam.vep.vep.regulatory_feature_consequences,\n",
    "                             family_non_ref = hl.agg.filter(hl.any(fam.GT.is_non_ref()), hl.agg.collect(fam.phenotypes.family))[0])\n",
    "    fam = fam.select_entries(fam.GT)\n",
    "    #fam.make_table().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t')\n",
    "    fam.rows().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t')\n",
    "    fams_to_export_all_other_treshold.append(pd.read_csv('/net/scratch/people/plggosborcz/temp-mts/temp.csv', sep ='\\t')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 20:53:18 Hail: INFO: merging 51 files totalling 244.7K...9 + 1) / 50]\n",
      "2022-05-21 20:53:18 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 48.665ms\n",
      "2022-05-21 20:53:26 Hail: INFO: merging 51 files totalling 43.0K...49 + 1) / 50]\n",
      "2022-05-21 20:53:26 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.153ms\n",
      "2022-05-21 20:53:33 Hail: INFO: merging 51 files totalling 7.4K...(49 + 1) / 50]\n",
      "2022-05-21 20:53:33 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 46.333ms\n",
      "2022-05-21 20:53:41 Hail: INFO: merging 51 files totalling 179.8K...9 + 1) / 50]\n",
      "2022-05-21 20:53:41 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.211ms\n",
      "2022-05-21 20:53:48 Hail: INFO: merging 51 files totalling 183.2K...9 + 1) / 50]\n",
      "2022-05-21 20:53:49 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 222.537ms\n",
      "2022-05-21 20:53:56 Hail: INFO: merging 51 files totalling 313... (49 + 1) / 50]\n",
      "2022-05-21 20:53:56 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.180ms\n",
      "2022-05-21 20:54:03 Hail: INFO: merging 51 files totalling 426.0K...7 + 3) / 50]\n",
      "2022-05-21 20:54:03 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.191ms\n",
      "2022-05-21 20:54:11 Hail: INFO: merging 51 files totalling 164.6K...9 + 1) / 50]\n",
      "2022-05-21 20:54:11 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.466ms\n",
      "2022-05-21 20:54:18 Hail: INFO: merging 51 files totalling 703.0K...8 + 2) / 50]\n",
      "2022-05-21 20:54:18 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.243ms\n",
      "2022-05-21 20:54:26 Hail: INFO: merging 51 files totalling 199.7K...7 + 3) / 50]\n",
      "2022-05-21 20:54:26 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.127ms\n",
      "2022-05-21 20:54:33 Hail: INFO: merging 51 files totalling 231.1K...8 + 2) / 50]\n",
      "2022-05-21 20:54:34 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.883ms\n",
      "2022-05-21 20:54:42 Hail: INFO: merging 51 files totalling 254.3K...9 + 1) / 50]\n",
      "2022-05-21 20:54:42 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 51.513ms\n",
      "2022-05-21 20:54:50 Hail: INFO: merging 51 files totalling 28.2K...49 + 1) / 50]\n",
      "2022-05-21 20:54:50 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 46.594ms\n",
      "2022-05-21 20:54:58 Hail: INFO: merging 51 files totalling 449.9K...8 + 2) / 50]\n",
      "2022-05-21 20:54:58 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.534ms\n",
      "2022-05-21 20:55:05 Hail: INFO: merging 51 files totalling 39.0K...49 + 1) / 50]\n",
      "2022-05-21 20:55:05 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 45.838ms\n",
      "2022-05-21 20:55:13 Hail: INFO: merging 51 files totalling 278.7K...9 + 1) / 50]\n",
      "2022-05-21 20:55:13 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.037ms\n",
      "2022-05-21 20:55:20 Hail: INFO: merging 51 files totalling 200.5K...9 + 1) / 50]\n",
      "2022-05-21 20:55:20 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.388ms\n",
      "2022-05-21 20:55:27 Hail: INFO: merging 51 files totalling 295.9K...9 + 1) / 50]\n",
      "2022-05-21 20:55:28 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 48.868ms\n",
      "2022-05-21 20:55:35 Hail: INFO: merging 51 files totalling 238.2K...7 + 3) / 50]\n",
      "2022-05-21 20:55:35 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 48.130ms\n"
     ]
    }
   ],
   "source": [
    "for fam in fams2:\n",
    "    fam = fam.drop(fam['a_index'], fam['was_split'])\n",
    "    fam = fam.transmute_rows(DP_stats = fam.dp_qc,\n",
    "                             GQ_stats =fam.gq_qc, \n",
    "                             gnomad_v3_nfe_af = fam.gnomad_v_3_1.freq.AF[2],\n",
    "                             gnomad_v3_nfe_homozygote_count = fam.gnomad_v_3_1.freq.homozygote_count[2],\n",
    "                             cadd = fam.cadd.score_phred,\n",
    "                             within_gene = fam.within_gene,\n",
    "                             hpo = fam.hpo,\n",
    "                             non_refs_healthy = fam.segregation.get('NO', 0),\n",
    "                             non_refs_gts = fam.segregation.get('YES', 0),\n",
    "                             most_severe_consequence = fam.vep.vep.most_severe_consequence,\n",
    "                             transcript_consequences = fam.vep.vep.transcript_consequences,\n",
    "                             intergenic_consequences = fam.vep.vep.intergenic_consequences,\n",
    "                             motif_feature_consequences = fam.vep.vep.motif_feature_consequences,\n",
    "                             regulatory_feature_consequences = fam.vep.vep.regulatory_feature_consequences,\n",
    "                             family_non_ref = hl.agg.filter(hl.any(fam.GT.is_non_ref()), hl.agg.collect(fam.phenotypes.family))[0])\n",
    "\n",
    "    #fam.make_table().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t')\n",
    "    fam.rows().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t') #this is for joint tables export\n",
    "    fams_to_export_selected.append(pd.read_csv('/net/scratch/people/plggosborcz/temp-mts/temp.csv', sep ='\\t')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 20:55:42 Hail: INFO: merging 51 files totalling 1.1M...(48 + 2) / 50]\n",
      "2022-05-21 20:55:42 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 53.350ms\n",
      "2022-05-21 20:55:50 Hail: INFO: merging 51 files totalling 73.8K...49 + 1) / 50]\n",
      "2022-05-21 20:55:50 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 45.603ms\n",
      "2022-05-21 20:55:57 Hail: INFO: merging 51 files totalling 52.4K...49 + 1) / 50]\n",
      "2022-05-21 20:55:57 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.763ms\n",
      "2022-05-21 20:56:05 Hail: INFO: merging 51 files totalling 735.0K...9 + 1) / 50]\n",
      "2022-05-21 20:56:05 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.860ms\n",
      "2022-05-21 20:56:12 Hail: INFO: merging 51 files totalling 908.5K...7 + 3) / 50]\n",
      "2022-05-21 20:56:12 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 48.166ms\n",
      "2022-05-21 20:56:20 Hail: INFO: merging 51 files totalling 76.9K...49 + 1) / 50]\n",
      "2022-05-21 20:56:20 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 48.426ms\n",
      "2022-05-21 20:56:27 Hail: INFO: merging 51 files totalling 2.4M...(48 + 2) / 50]\n",
      "2022-05-21 20:56:27 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 55.157ms\n",
      "2022-05-21 20:56:35 Hail: INFO: merging 51 files totalling 978.6K...8 + 2) / 50]\n",
      "2022-05-21 20:56:35 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.530ms\n",
      "2022-05-21 20:56:42 Hail: INFO: merging 51 files totalling 3.2M...(48 + 2) / 50]\n",
      "2022-05-21 20:56:42 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 57.321ms\n",
      "2022-05-21 20:56:50 Hail: INFO: merging 51 files totalling 390.1K...7 + 3) / 50]\n",
      "2022-05-21 20:56:50 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 47.992ms\n",
      "2022-05-21 20:56:57 Hail: INFO: merging 51 files totalling 1.1M...(47 + 3) / 50]\n",
      "2022-05-21 20:56:57 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 50.972ms\n",
      "2022-05-21 20:57:06 Hail: INFO: merging 51 files totalling 1.1M...(49 + 1) / 50]\n",
      "2022-05-21 20:57:06 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 50.555ms\n",
      "2022-05-21 20:57:13 Hail: INFO: merging 51 files totalling 173.6K...9 + 1) / 50]\n",
      "2022-05-21 20:57:13 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 46.559ms\n",
      "2022-05-21 20:57:21 Hail: INFO: merging 51 files totalling 2.6M...(49 + 1) / 50]\n",
      "2022-05-21 20:57:21 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 53.512ms\n",
      "2022-05-21 20:57:28 Hail: INFO: merging 51 files totalling 365.9K...9 + 1) / 50]\n",
      "2022-05-21 20:57:28 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.430ms\n",
      "2022-05-21 20:57:36 Hail: INFO: merging 51 files totalling 1.4M...(48 + 2) / 50]\n",
      "2022-05-21 20:57:36 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 51.919ms\n",
      "2022-05-21 20:57:44 Hail: INFO: merging 51 files totalling 692.8K...9 + 1) / 50]\n",
      "2022-05-21 20:57:44 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 51.442ms\n",
      "2022-05-21 20:57:51 Hail: INFO: merging 51 files totalling 1.6M...(49 + 1) / 50]\n",
      "2022-05-21 20:57:51 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 51.345ms\n",
      "2022-05-21 20:57:58 Hail: INFO: merging 51 files totalling 1.1M...(49 + 1) / 50]\n",
      "2022-05-21 20:57:58 Hail: INFO: while writing:\n",
      "    /net/scratch/people/plggosborcz/temp-mts/temp.csv\n",
      "  merge time: 49.760ms\n"
     ]
    }
   ],
   "source": [
    "for fam in fams4:\n",
    "    fam = fam.drop(fam['a_index'], fam['was_split'])\n",
    "    fam = fam.transmute_rows(DP_stats = fam.dp_qc,\n",
    "                             GQ_stats =fam.gq_qc, \n",
    "                             gnomad_v3_nfe_af = fam.gnomad_v_3_1.freq.AF[2],\n",
    "                             gnomad_v3_nfe_homozygote_count = fam.gnomad_v_3_1.freq.homozygote_count[2],\n",
    "                             cadd = fam.cadd.score_phred,\n",
    "                             within_gene = fam.within_gene,\n",
    "                             hpo = fam.hpo,\n",
    "                             non_refs_healthy = fam.segregation.get('NO', 0),\n",
    "                             non_refs_gts = fam.segregation.get('YES', 0),\n",
    "                             most_severe_consequence = fam.vep.vep.most_severe_consequence,\n",
    "                             transcript_consequences = fam.vep.vep.transcript_consequences,\n",
    "                             intergenic_consequences = fam.vep.vep.intergenic_consequences,\n",
    "                             motif_feature_consequences = fam.vep.vep.motif_feature_consequences,\n",
    "                             regulatory_feature_consequences = fam.vep.vep.regulatory_feature_consequences,\n",
    "                             family_non_ref = hl.agg.filter(hl.any(fam.GT.is_non_ref()), hl.agg.collect(fam.phenotypes.family))[0])\n",
    "    \n",
    "    #fam.make_table().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t')\n",
    "    fam.rows().export('/net/scratch/people/plggosborcz/temp-mts/temp.csv', delimiter='\\t') #this is for joint tables export\n",
    "    fams_to_export_selected_other_treshold.append(pd.read_csv('/net/scratch/people/plggosborcz/temp-mts/temp.csv', sep ='\\t')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat(fams_to_export_selected[0:19]).to_csv('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/families-05-2022-schema-duplicated-selected.csv')\n",
    "pd.concat(fams_to_export_all[0:19]).to_csv('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/familes-all-5-percent.csv')\n",
    "pd.concat(fams_to_export_selected_other_treshold[0:19]).to_csv('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/familes-all-1-percent.csv')\n",
    "#pd.concat(fams_to_export_all_other_treshold[0:19]).to_csv('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/families-05-2022-schema-duplicateds-all-other.csv')\n",
    "\n",
    "#this gives concatenated info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of duplicated genes - optional\n",
    "results_all_genes = pd.concat(fams_to_export_all[0:19])['within_gene']\n",
    "results_all_genes = [item for sublist in results_all_genes for item in sublist] \n",
    "list(set(results_all_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tables = [fams_to_export_all,\n",
    "                  fams_to_export_all_other_treshold, \n",
    "                  fams_to_export_selected,\n",
    "                  fams_to_export_selected_other_treshold,\n",
    "                  descriptions_to_export]\n",
    "concatenated_list = [val for tup in zip(*list_of_tables) for val in tup]\n",
    "\n",
    "sheet_names = [fams_all_genes,\n",
    "               fams_all_genes_other_treshold,\n",
    "               fams_selected_genes,\n",
    "               fams_selected_genes_other_treshold,\n",
    "               fams_description]\n",
    "\n",
    "concatenated_list_sheets = [val for tup in zip(*sheet_names) for val in tup]\n",
    "\n",
    "with pd.ExcelWriter('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/families-05-2022-schema.xlsx') as writer:\n",
    "    for idx, res in enumerate(concatenated_list):\n",
    "        res.to_excel(writer, sheet_name=str(concatenated_list_sheets[idx]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find variants that are common across multiple families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loci_all = []\n",
    "loci_selected = []\n",
    "\n",
    "for f in fams_to_export_all:\n",
    "    loci_all.append(list(f['locus.contig']+':'+f['locus.position'].astype(str)))\n",
    "    f['locus'] = f['locus.contig']+':'+f['locus.position'].astype(str)\n",
    "    \n",
    "for f in fams_to_export_selected:\n",
    "    loci_selected.append(list(f['locus.contig']+':'+f['locus.position'].astype(str)))\n",
    "    f['locus'] = f['locus.contig']+':'+f['locus.position'].astype(str)\n",
    "\n",
    "loci_all = [item for sublist in loci_all for item in sublist]\n",
    "loci_selected = [item for sublist in loci_selected for item in sublist]\n",
    "\n",
    "seen = set()\n",
    "dupes_loci_all = [x for x in loci_all if x in seen or seen.add(x)]    \n",
    "\n",
    "seen = set()\n",
    "dupes_loci_selected = [x for x in loci_selected if x in seen or seen.add(x)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "dupes_loci_all = [x for x in loci_all if x in seen or seen.add(x)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupes_loci_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fams_to_export_all_filtered = []\n",
    "\n",
    "for f in fams_to_export_all:\n",
    "    f_filtered = f[f.locus.isin(dupes_loci_all)]\n",
    "    fams_to_export_all_filtered.append(f_filtered)\n",
    "    \n",
    "fams_to_export_selected_filtered = []\n",
    "\n",
    "for f in fams_to_export_selected:\n",
    "    f_filtered = f[f.locus.isin(dupes_loci_selected)]\n",
    "    fams_to_export_selected_filtered.append(f_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tables = [fams_to_export_all_filtered, fams_to_export_selected_filtered, descriptions_to_export]\n",
    "concatenated_list = [val for tup in zip(*list_of_tables) for val in tup]\n",
    "\n",
    "sheet_names = [fams_all_genes, fams_selected_genes, fams_description]\n",
    "concatenated_list_sheets = [val for tup in zip(*sheet_names) for val in tup]\n",
    "\n",
    "with pd.ExcelWriter('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/families-11-2021-schema-duplicated-variants.xlsx') as writer:\n",
    "    for idx, res in enumerate(concatenated_list):\n",
    "        res.to_excel(writer, sheet_name=str(concatenated_list_sheets[idx]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some extra code (variant overrepresentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fams = ['A', 'B', 'C','D', 'E', 'F', 'G', 'I', 'J', 'R', 'S', 'U', 'W', 'X', 'Y'] # h and t fams are excluded as they have two schemas of heredity\n",
    "fams_all_genes = ['A_all_genes', 'B_all_genes', 'C_all_genes','D_all_genes',\n",
    "                  'E_all_genes', 'F_all_genes', 'G_all_genes',\n",
    "                  'I_all_genes', 'J_all_genes', 'R_all_genes', 'S_all_genes',\n",
    "                  'U_all_genes', 'W_all_genes', 'X_all_genes', 'Y_all_genes']\n",
    "fams_selected_genes = ['A_selected_genes', 'B_selected_genes', 'C_selected_genes','D_selected_genes',\n",
    "                  'E_selected_genes', 'F_selected_genes', 'G_selected_genes',\n",
    "                  'I_selected_genes', 'J_selected_genes', 'R_selected_genes', 'S_selected_genes',\n",
    "                  'U_selected_genes', 'W_selected_genes', 'X_selected_genes', 'Y_selected_genes']\n",
    "fams_description = ['A_description', 'B_description', 'C_description','D_description',\n",
    "                  'E_description', 'F_description', 'G_description',\n",
    "                  'I_description', 'J_description', 'R_description', 'S_description',\n",
    "                  'U_description', 'W_description', 'X_description', 'Y_description']\n",
    "\n",
    "fams1 = []\n",
    "fams2 = []\n",
    "fams_to_export_all = []\n",
    "fams_to_export_selected = []\n",
    "descriptions_to_export = []\n",
    "\n",
    "genes_fams1 = []\n",
    "genes_fams2 = []\n",
    "\n",
    "for f in fams:\n",
    "    \n",
    "    fam = hl.read_matrix_table('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fam-vep'+f+'.mt')\n",
    "    ctrl = fam.aggregate_cols(hl.agg.counter(fam.phenotypes.disease))['NO']\n",
    "    gts = fam.aggregate_cols(hl.agg.counter(fam.phenotypes.disease))['YES']\n",
    "\n",
    "    fam = fam.annotate_rows(segregation = hl.agg.group_by(fam.phenotypes.disease, hl.agg.count_where(fam.GT.is_non_ref())))\n",
    "    fam = fam.filter_rows(((fam.segregation.get('NO', 0))/ctrl < 0.33) & ((fam.segregation.get('YES', 0))/gts > 0.66))\n",
    "    \n",
    "    fam = fam.filter_rows(hl.is_snp(fam.alleles[0], fam.alleles[1]))\n",
    "    \n",
    "    fam1 = fam.filter_rows(hl.if_else(hl.is_defined(fam.gnomad_v_3_1.freq.AF[2]), fam.gnomad_v_3_1.freq.AF[2] < 0.001, True))\n",
    "    genes_fams1.append(fam1.within_gene.collect())\n",
    "    fams1.append(fam1)\n",
    "    \n",
    "    fam2 = fam.filter_rows((hl.any(lambda x: hl.literal(all_genes).contains(x), fam.within_gene)))\n",
    "    fam2 = fam2.filter_rows(hl.if_else(hl.is_defined(fam2.gnomad_v_3_1.freq.AF[2]), fam2.gnomad_v_3_1.freq.AF[2] < 0.05, True))\n",
    "    genes_fams2.append(fam2.within_gene.collect())\n",
    "    fams2.append(fam2)\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams1):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family\n",
    "    genes_fams1[idx] = list(set(gene_list))\n",
    "\n",
    "genes_fams1 = [item for sublist in genes_fams1 for item in sublist]\n",
    "\n",
    "seen = set()\n",
    "dupes_1 = [x for x in genes_fams1 if x in seen or seen.add(x)]    \n",
    "\n",
    "len(set(dupes_1))\n",
    "\n",
    "# optional - export only genes with multiple variants:\n",
    "\n",
    "for idx, fam in enumerate(fams1):\n",
    "    fam = fam.filter_rows((hl.any(lambda x: hl.literal(dupes_1).contains(x), fam.within_gene)))\n",
    "    fams1[idx] = fam\n",
    "\n",
    "for idx, gene_list in enumerate(genes_fams2):\n",
    "    gene_list = [item for sublist in gene_list for item in sublist] # first flatten the gene list in each family\n",
    "    genes_fams2[idx] = list(set(gene_list))\n",
    "\n",
    "genes_fams2 = [item for sublist in genes_fams2 for item in sublist]\n",
    "\n",
    "len(set(genes_fams2))\n",
    "\n",
    "seen = set()\n",
    "dupes_2 = [x for x in genes_fams2 if x in seen or seen.add(x)]    \n",
    "\n",
    "len(set(dupes_2))\n",
    "\n",
    "for idx, fam in enumerate(fams2):\n",
    "    fam = fam.filter_rows((hl.any(lambda x: hl.literal(dupes_2).contains(x), fam.within_gene)))\n",
    "    fams2[idx] = fam\n",
    "\n",
    "for fam in fams1:\n",
    "    fam = fam.drop(fam['a_index'], fam['was_split'])\n",
    "    fam = fam.transmute_rows(DP_stats = fam.dp_qc,\n",
    "                             GQ_stats =fam.gq_qc, \n",
    "                             gnomad_v3_nfe_af = fam.gnomad_v_3_1.freq.AF[2],\n",
    "                             gnomad_v3_nfe_homozygote_count = fam.gnomad_v_3_1.freq.homozygote_count[2],\n",
    "                             cadd = fam.cadd.cadd_score,\n",
    "                             within_gene = fam.within_gene,\n",
    "                             hpo = fam.hpo,\n",
    "                             non_refs_healthy = fam.segregation.get('NO', 0),\n",
    "                             non_refs_gts = fam.segregation.get('YES', 0),\n",
    "                             most_severe_consequence = fam.vep.vep.most_severe_consequence,\n",
    "                             transcript_consequences = fam.vep.vep.transcript_consequences,\n",
    "                             intergenic_consequences = fam.vep.vep.intergenic_consequences,\n",
    "                             motif_feature_consequences = fam.vep.vep.motif_feature_consequences,\n",
    "                             regulatory_feature_consequences = fam.vep.vep.regulatory_feature_consequences)\n",
    "\n",
    "    fam = fam.select_entries(fam.GT)\n",
    "    fams_to_export_all.append(fam.make_table().to_pandas())\n",
    "    descriptions_to_export.append(fam.cols().to_pandas())\n",
    "\n",
    "for idx, f in enumerate(fams_to_export_all):\n",
    "    fams_to_export_all[idx] = f[f.columns.drop(list(f.filter(regex='.phased')))]\n",
    "    \n",
    "    \n",
    "\n",
    "for fam in fams2:\n",
    "    fam = fam.drop(fam['a_index'], fam['was_split'])\n",
    "    fam = fam.transmute_rows(DP_stats = fam.dp_qc,\n",
    "                             GQ_stats =fam.gq_qc, \n",
    "                             gnomad_v3_nfe_af = fam.gnomad_v_3_1.freq.AF[2],\n",
    "                             gnomad_v3_nfe_homozygote_count = fam.gnomad_v_3_1.freq.homozygote_count[2],\n",
    "                             cadd = fam.cadd.cadd_score,\n",
    "                             within_gene = fam.within_gene,\n",
    "                             hpo = fam.hpo,\n",
    "                             non_refs_healthy = fam.segregation.get('NO', 0),\n",
    "                             non_refs_gts = fam.segregation.get('YES', 0),\n",
    "                             most_severe_consequence = fam.vep.vep.most_severe_consequence,\n",
    "                             transcript_consequences = fam.vep.vep.transcript_consequences,\n",
    "                             intergenic_consequences = fam.vep.vep.intergenic_consequences,\n",
    "                             motif_feature_consequences = fam.vep.vep.motif_feature_consequences,\n",
    "                             regulatory_feature_consequences = fam.vep.vep.regulatory_feature_consequences)\n",
    "    \n",
    "    fam = fam.select_entries(fam.GT)\n",
    "    fams_to_export_selected.append(fam.make_table().to_pandas())\n",
    "\n",
    "for idx, f in enumerate(fams_to_export_selected):\n",
    "    fams_to_export_selected[idx] = f[f.columns.drop(list(f.filter(regex='.phased')))]\n",
    "\n",
    "list_of_tables = [fams_to_export_all, fams_to_export_selected, descriptions_to_export]\n",
    "concatenated_list = [val for tup in zip(*list_of_tables) for val in tup]\n",
    "\n",
    "sheet_names = [fams_all_genes, fams_selected_genes, fams_description]\n",
    "concatenated_list_sheets = [val for tup in zip(*sheet_names) for val in tup]\n",
    "\n",
    "with pd.ExcelWriter('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/families-11-2021-duplicate-genes.xlsx') as writer:\n",
    "    for idx, res in enumerate(concatenated_list):\n",
    "        res.to_excel(writer, sheet_name=str(concatenated_list_sheets[idx]), header=True, index=False)\n",
    "\n",
    "loci_all = []\n",
    "loci_selected = []\n",
    "\n",
    "for f in fams_to_export_all:\n",
    "    loci_all.append(list(f['locus.contig']+':'+f['locus.position'].astype(str)))\n",
    "    f['locus'] = f['locus.contig']+':'+f['locus.position'].astype(str)\n",
    "    \n",
    "for f in fams_to_export_selected:\n",
    "    loci_selected.append(list(f['locus.contig']+':'+f['locus.position'].astype(str)))\n",
    "    f['locus'] = f['locus.contig']+':'+f['locus.position'].astype(str)\n",
    "\n",
    "loci_all = [item for sublist in loci_all for item in sublist]\n",
    "loci_selected = [item for sublist in loci_selected for item in sublist]\n",
    "\n",
    "seen = set()\n",
    "dupes_loci_all = [x for x in loci_all if x in seen or seen.add(x)]    \n",
    "\n",
    "seen = set()\n",
    "dupes_loci_selected = [x for x in loci_all if x in seen or seen.add(x)]    \n",
    "\n",
    "fams_to_export_all_filtered = []\n",
    "\n",
    "for f in fams_to_export_all:\n",
    "    f_filtered = f[f.locus.isin(dupes_loci_all)]\n",
    "    fams_to_export_all_filtered.append(f_filtered)\n",
    "    \n",
    "fams_to_export_selected_filtered = []\n",
    "\n",
    "for f in fams_to_export_selected:\n",
    "    f_filtered = f[f.locus.isin(dupes_loci_selected)]\n",
    "    fams_to_export_selected_filtered.append(f_filtered)\n",
    "\n",
    "list_of_tables = [fams_to_export_all_filtered, fams_to_export_selected_filtered, descriptions_to_export]\n",
    "concatenated_list = [val for tup in zip(*list_of_tables) for val in tup]\n",
    "\n",
    "sheet_names = [fams_all_genes, fams_selected_genes, fams_description]\n",
    "concatenated_list_sheets = [val for tup in zip(*sheet_names) for val in tup]\n",
    "\n",
    "with pd.ExcelWriter('/net/archive/groups/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/families-11-2021-duplicated-variants.xlsx') as writer:\n",
    "    for idx, res in enumerate(concatenated_list):\n",
    "        res.to_excel(writer, sheet_name=str(concatenated_list_sheets[idx]), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Investigate all carriers of selected mutations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.read_matrix_table('/net/pr2/projects/plgrid/plggneuromol/imdik-zekanowski-gts/data/joint-gts-only/fams-anno.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15760027, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = ['chr19:53803962:G:A',\n",
    "            'chr5:76482747:A:C',\n",
    "            'chr4:109086168:T:A',\n",
    "            'chr1:223300619:G:A',\n",
    "            'chr12:120368947:C:A',\n",
    "            'chr1:31727178:G:A',\n",
    "            'chr12:10723010:G:T',\n",
    "            'chr11:111020375:C:A',\n",
    "            'chr6:101375096:A:G',\n",
    "            'chr12:97272797:T:A',\n",
    "            'chr7:713068:C:T',\n",
    "            'chr1:121379326:G:A',\n",
    "            'chr10:74875378:T:C',\n",
    "            'chr6:17649263:G:A',\n",
    "            'chr7:106112481:G:A',\n",
    "            'chr9:38396880:C:T',\n",
    "            'chr18:12702512:G:A',\n",
    "            'chr12:2855156:C:T',\n",
    "            'chr11:113697354:C:T',\n",
    "            'chr1:117120939:G:T',\n",
    "            'chr3:149073278:C:A',\n",
    "            'chr4:81459515:C:T',\n",
    "            'chr14:96264526:C:T',\n",
    "            'chr3:53855341:G:A',\n",
    "            'chr7:2569000:G:A',\n",
    "            'chr10:94687805:T:A',\n",
    "            'chr4:88417579:G:T',\n",
    "            'chr17:69026976:C:T',\n",
    "            'chr16:2317763:T:A',\n",
    "            'chr3:132601106:C:T',\n",
    "            'chr12:100396328:A:C',\n",
    "            'chr12:101319592:G:A',\n",
    "            'chr12:1885984:G:A',\n",
    "            'chr3:49004877:C:T',\n",
    "            'chr17:10631666:T:C',\n",
    "            'chr15:101324994:G:A',\n",
    "            'chr16:4883689:G:A',\n",
    "            'chr17:28385146:T:C',\n",
    "            'chr16:89727323:C:T',\n",
    "            'chr12:101677317:G:A',\n",
    "            'chr12:111803962:G:A',\n",
    "            'chr12:94279637:G:A',\n",
    "            'chr12:57244143:G:A',\n",
    "            'chr6:39861001:A:G',\n",
    "            'chr8:99999384:G:A',\n",
    "            'chr21:46114063:G:A',\n",
    "            'chr2:108499560:A:G',\n",
    "            'chr3:49652822:G:C',\n",
    "            'chr16:80549649:C:G',\n",
    "            'chr7:107916945:T:C',\n",
    "            'chr9:37541696:G:A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,v in enumerate(variants):\n",
    "    variants[idx] = hl.parse_variant(v, reference_genome='GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_to_filter = hl.utils.range_table(len(variants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_to_filter = variants_to_filter.annotate(\n",
    "    locus = hl.literal(variants)[variants_to_filter.idx]['locus'],\n",
    "    alleles = hl.literal(variants)[variants_to_filter.idx]['alleles']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_to_filter = variants_to_filter.key_by(\n",
    "    variants_to_filter.locus, variants_to_filter.alleles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variants_to_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.filter_rows(hl.is_defined(variants_to_filter[mt.row_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 16:15:51.446 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "[Stage 8:==============================>                      (577 + 12) / 1000]\r"
     ]
    }
   ],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_rows(\n",
    "    samples_het = hl.agg.filter(mt.GT.is_het(), hl.agg.collect(mt.s)),\n",
    "    samples_hom_var = hl.agg.filter(mt.GT.is_hom_var(), hl.agg.collect(mt.s))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.select(mt.samples_het, mt.samples_hom_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.export(\n",
    "    '/net/pr2/projects/plgrid/plggneuromol/imdik-zekanowski-gts/results/burden-and-family/samples_non_ref.csv', delimiter='\\t'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
